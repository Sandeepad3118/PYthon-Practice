# Tuesday, July 01 2025

## Tasks
. finish gen ai course - should (Goal : finish the video, if possible code sth) - https://www.youtube.com/watch?v=d4yCWBGFCEs&t=403s
. contractual jobs - 20 jobs --> need to check what kind of jobs out there so i can postpone to later part of tomrrw
. call mohmmad - not going through
. indian options - putting on hold


Niche tech:
cloud - devsecops, devops, architect, security
ai applications
security

## Notes

Goal for doing the ai course : 
- have an idea about proceedings in ai industry and have an idea of where things are headed. just focus currently on it as a side project. dont over indulge
- when you see the code you have to understand why it is there exactly
- also see where cloud and other branches come into picture. what industries are going to be in demand in future

First go through without coding - 3 poc's
. news article urls - gpt usage
. 
. 

stop here and then when having time try to code it in pycharm and juptyer

----------------------

Overview 
00:00

This Gen AI course covers the fundamentals of Gen AI, the Lang chain python framework for building Gen applications, and the development of two end-to-end Gen AI projects 
00:00.


The course will explore two projects, the first using a commercial GPT model to build an equity news research tool, and the second using an open-source LLM model to build a Q&A tool for the retail industry 
00:17.


Gen AI is a category of AI that can be categorized into two sections, one of which is generative, and understanding this definition is the starting point for the course 
00:00.


What is Gen AI or Generative AI? 
00:35

Non-generative AI deals with problems such as analyzing a chest x-ray to determine if a person has pneumonia or evaluating a person's credit history to decide if they should be given a loan, where existing data is used to make decisions without creating new content 
00:35.


In non-generative AI problems, the focus is on making certain decisions based on the available data, rather than generating new information 
00:55.


Generative AI, on the other hand, is a category of AI that involves generating new content, as seen in applications like Chat GPD, where users can create new text, plan trips, or even generate images 
01:12.


Gen AI evolution 
01:25

Content can be in various forms such as text, images, video, audio, etc., and the evolution of AI involved solving problems like predicting home prices based on factors like area, bedrooms, and age, which is called statistical machine learning, with features being the determining factors 
01:25.


In image recognition, such as identifying a cat or dog, the features are complex, like whiskers or pointy ears, and the data is unstructured, unlike the structured data in house price prediction, which has features like area and bedrooms 
01:45.


The complexity of image detection led to the invention of neural networks, giving birth to deep learning, which was later followed by recurrent neural networks that can handle sequential data, like translating sentences from one language to another 
02:42.


Recurrent neural networks work by feeding the first word of a sentence to the network, then feeding the next word along with the translation of the previous word, creating a loop within the network, and this approach is used for solving problems like language translation 
03:16.


More sophisticated problems, like generative tasks, involve predicting the next word in a sequence, such as in email auto-completion, where the network reads the content of the email and predicts the probability of the next word, with probabilities generally between zero and one 
04:14.


Language models are AI models that can predict the next word or set of words for a given sequence of words, and they can be built by passing a huge corpus of text through the model, allowing it to understand the context and predict the next word in a sentence 
05:14.


The training of a neural network can be achieved by creating training pairs from a given text, such as a Wikipedia article, and using these pairs to train the network, with the goal of filling in missing words, a process known as self-supervised learning 
06:11.


This approach allows for the training of a language model without the need for a large amount of labeled data, as text from various sources like Wikipedia, news articles, and books can be used to generate training pairs 
06:30.


The neural network, once trained on a large amount of data, can predict missing words or the next word in a sentence, similar to how Gmail's autocomplete feature works, and can become a large language model like the one behind Chat GPT 
07:06.


The model behind Chat GPT is a large language model with 175 billion parameters, which are the weights in the neural network, and this was made possible by the development of the Transformer architecture, as described in the paper "Attention is All You Need" 
07:28.


The Transformer architecture is a powerful tool that has led to the development of various models, including Google's BERT and Open AI's GPT, which is a generative pre-trained Transformer 
08:23.


In addition to text models like GPT, there are also image models, such as Stable Diffusion, which can generate images from text prompts, and video models like Open ASR, which can generate videos from text prompts 
08:56.


The development of these models has been made possible by the Transformer architecture, and understanding these concepts, including large language models (LLM) and Vector DB, is important for working in the field of Gen AI 
09:36.


What is LLM (Large Language Model)? 
10:01

A stochastic parrot, like Buddy, is a system that uses statistical probability and randomness to predict the next word or set of words based on past conversations, and a language model works in a similar way, using technology like neural networks to make predictions 
10:39.


Language models can be trained on various datasets, such as movie-related articles from Wikipedia, and can be used in applications like Gmail autocomplete, which uses a language model to predict the next set of words in a sentence 
10:58.


A large language model (LLM) is trained on a huge volume of data, including Wikipedia articles, Google news articles, and online books, and can capture complex patterns and nuances in language, allowing it to complete tasks like writing a poem or giving nutrition advice 
11:55.


LLMs, like GPT3 or GPT4, use neural networks with trillions of parameters to make predictions, and other examples of LLMs include Palm2 by Google and Lama by Meta 
12:14.


In addition to statistical predictions, LLMs can also use reinforcement learning with human feedback (RLHF) to improve their performance, which involves human intervention to identify and correct toxic or undesirable responses 
12:50.


The use of RLHF allows LLMs to learn from human feedback and avoid using toxic language, as seen in the example of Buddy, the parrot, who learned to avoid toxic language after being trained by Peter 
13:14.


Despite their power, LLMs do not possess subjective experience, emotions, or consciousness like humans do, and they work purely based on the data they have been trained on 
13:49.


Embeddings, Vector Database 
13:55

Embeddings are a numeric representation of text in the form of a vector, allowing for the capture of the meaning of the text, and enabling mathematical operations such as "Paris minus France plus India equal to Delhi" or "apple minus Tim Cook plus Satya Nadella equal to Microsoft" 
14:15.


Vector databases are used to store these embeddings and perform efficient searches on them, with some AI startups raising millions of dollars in funding to develop this technology 
14:51.


Semantic search, used by Google, involves understanding the intent of a user query and using context to perform the search, rather than relying on exact keyword matching, and it uses the concept of embedding, such as word embedding or sentence embedding 
15:12.


Embeddings can be created by assigning values to different features or properties of words, resulting in a sequence of numbers, or a vector, that represents the word, such as the word "Apple" having properties like "related to phones" or "location" 
15:47.


The similarity between words can be determined by comparing their embeddings, with similar words having similar vector values, and complex arithmetic operations can be performed using techniques like word2vec 
16:42.


There are various techniques used to represent words, sentences, or documents as embeddings, including handcrafted features and complex statistical techniques, with some techniques being used in the development of chat GPT and Transformer-based models 
17:38.


Vector databases and embeddings have many applications, including semantic search, and are being developed by various AI startups, with the goal of improving the efficiency and accuracy of search results 
14:33.


Embedding techniques are becoming increasingly popular, and when using the Open AI API for embedding, it is essential to understand the technique being used underneath, especially when building text-based AI applications that involve storing thousands or millions of embedding vectors 
18:16.


Traditional relational databases, such as SQL databases, can be used to store embedding vectors, but this approach can be inefficient when dealing with millions of records, as it requires comparing the query vector with each stored vector using cosine similarity, which can be computationally expensive 
18:36.


To improve the efficiency of searching and storing embedding vectors, techniques such as locality sensitive hashing can be used, which involves creating buckets of similar-looking embeddings using a hashing function, allowing for faster search and comparison of vectors within the same bucket 
20:13.


Vector databases are designed to address the challenges of storing and searching large numbers of embedding vectors, providing faster search capabilities and optimal storage, making them a popular choice for applications that require efficient management of vector data 
20:47.


The use of indexing, similar to database indexing, can also help speed up the search process in vector databases, enabling faster retrieval of relevant documents and improving overall performance 
20:30.


Locality sensitive hashing is one of the techniques used in vector databases to enable faster search and storage, and there are many other techniques available, which can be explored further through additional resources, such as the article that will be provided 
21:08.


Retrieval Augmented Generation 
21:27

Retrieval Augmented Generation (RAG) is a concept that allows for fine-tuning chat GPT on private organizational data or public custom data sets, and it can be used to build chat GPT-like solutions on specific data sets 
21:27.


The concept of RAG can be explained using an analogy of a college student named Mera, who can either fully train in biology or use an open book exam approach to answer questions from a book, with the latter option being more time and cost-efficient 
22:03.


In building RAG-based applications, a large language model (LLM) refers to a database to pull answers, which can be from private internal organizational data or public data, and this approach saves time and money 
23:35.


The technical understanding of RAG will be covered in an end-to-end project, and tools such as Chat GPT, which uses GP4 as a large language model, can be used for building Gen AI applications 
24:14.


GP4 is a model similar to the human brain, trained on vast amounts of data, including Wikipedia articles and books, and it needs a backend server to interact and solve problems 
24:34.


A backend server can be built using nodejs, Jango, or fast API, and it can be deployed to Azure Cloud or other services, allowing for the use of retrieval augmented generation to pull answers from databases 
25:30.


There are commercial models such as GPT and Jini, as well as open-source models like MRA, Lama, and others, and frameworks like Langchain can be used to build LLM applications and switch between different models 
27:04.


Langchain is a Python framework that makes building LLM apps easier and allows for switching between different models, such as GP4 and Lama 2, without having to redo large amounts of code 
28:01.


Tooling for Gen AI 
28:21

To work with Gen AI, a model such as a commercial model like GP4 or an open-source model like LLaMA or MISTral is required, and it can also be an image model, with options including cloud services like Azure, Open AI, Amazon, Badrock, or Google Cloud, and a framework like Lang Chain, which can utilize the Hugging Face Transformer library to access various open-source models 
28:21.


In addition to a model and cloud service, a framework and deep learning libraries like PyTorch or TensorFlow are necessary for Gen AI tooling, and Lang Chain is a suitable framework for building Gen AI applications 
28:38.


Lang Chain is a framework that enables the building of Gen AI applications, and there is a crash course available that covers the Lang Chain framework in detail, providing an in-depth introduction to its capabilities and usage 
28:57.


The Lang Chain framework is a key component of Gen AI tooling, allowing users to build and develop Gen AI applications, and it is compatible with various deep learning libraries and open-source models, making it a versatile and useful tool for Gen AI development 
29:12.


Langchain Fundamentals 
29:18

The application on top of a large language model, such as Lang chain, will be discussed, and a restaurant idea generator application using Streamlet will be built, where users can input a cuisine and generate a fancy restaurant name along with menu items 
29:18.


Lang chain addresses the problem of building applications using large language models, such as GPT 3.5 or 4, and provides a framework for integrating these models with other data sources, like Google search or internal organization databases 
29:55.


The restaurant idea generator application is an example of an LLM-based application, where users can input a cuisine, such as Indian or Mexican, and generate a fancy name and menu items, but this approach has limitations, including cost and limited access to internal organization data 
30:14.


The limitations of using Open AI API include the cost associated with calling the API, limited knowledge cutoff, and lack of access to internal organization data, which is why businesses want to build their own LLM-based applications 
31:33.


Lang chain provides a framework for building LLM-based applications, allowing for plug-and-play support with various models, such as Open AI GPT 3 or 4, or open-source models like Hugging Face Bloom, and integration with external data sources 
33:04.


The Lang chain framework enables applications to pull information from various sources, including Google search, Wikipedia, or internal organization databases, making it a useful tool for building LLM-based applications 
33:24.


To build applications using a large language model (LLM), a framework called Lang Chain is utilized, and the initial setup involves creating an account on Open AI by logging in with Google or individual email credentials, then obtaining an API key, which looks like "SK-something" and is used in the code, 
33:43.


The API key is found in the account dashboard under "Manage Account" and "API Keys", and it is recommended to create separate keys for separate projects, with the option to generate a new key if needed, 
34:01.


The obtained API key is then stored in a secure place, such as an environment variable using the OS module, and can be imported and set in the code, 
34:41.


The next step involves installing the necessary modules, including Lang Chain and Open AI, using pip install, and then importing the LLM called Open AI from Lang Chain, 
35:01.


The Open AI model has a variable called temperature, which determines the level of creativity, with a temperature of 0 being very safe and 1 being more creative, and the model can be used to generate text based on a given question or prompt, 
36:22.


A prompt template can be created using Lang Chain's prompts, allowing for the passage of variables such as input variables, and the template can be customized to generate specific text based on the input, 
37:42.


The LLM can be used to generate creative text, such as coming up with a fancy name for a restaurant, and the output will vary based on the input and the temperature setting, 
38:01.


The prompt template can be used to simplify the process of generating text, by passing variables and using a predefined template, 
38:18.


The concept of a prompt template is introduced, which allows for the creation of a template with a variable, such as a cuisine type, to generate different restaurant names, for example, by replacing "Italian" with a variable "kuin" and using a template called "prompt template name" to format the output 
38:38.


The Lang chain framework is utilized, which includes an object called "chain" that enables the creation of a simple object where the LLM and prompt template are defined, allowing for the generation of restaurant names by passing the cuisine variable, such as "Mexican" or "American", without having to pass the whole sentence 
39:37.


The chain object can be used to call the OpenAI API internally, and it is mentioned that when creating an OpenAI account, $5 free credit is provided, which is sufficient for initial learning and exploration 
40:19.


A sequential chain is introduced, which allows for the generation of menu items for a restaurant based on the restaurant name, with the input of the second chain being the output of the first chain, and an example is given where the input is the restaurant name and the output is a list of suggested food menu items 
41:17.


The concept of a simple sequential chain is explained, where one input and one output are used, and intermediate steps can be added where the input of the second step is the output of the first step, and an example is given where two chains are created, one for generating the restaurant name and another for generating food menu items 
41:55.


The simple sequential chain is coded, and it is explained that the order of the chains matters, with the restaurant name chain coming first, followed by the food item chain, and the chain is run to generate the output for a specific cuisine type, such as "Indian" 
43:13.


To generate a restaurant name and menu items, a sequential chain can be used, which can have multiple input and output variables, allowing for more complex tasks such as generating a restaurant name and menu items for a specific cuisine like Mexican or Indian 
44:47.


The sequential chain takes parameters such as chains and input variables, and the output variables can be specified to include the restaurant name and menu items, which can be called using a dictionary with the cuisine as the input 
46:11.


When running the chain, a dictionary is used to provide the input variables, and the output includes both the input and the generated restaurant name and menu items, such as "Arabian Bistro" with menu items like "Hummus with P bread falafal" 
46:56.


The code written so far can be used to create a Streamlit-based application for a restaurant name generator, which allows data scientists to build proof-of-concept applications quickly without needing frontend frameworks like ReactJS 
47:17.


To use Streamlit, the library needs to be installed using "pip install streamlit" before creating an application, and a simple application can be created with a title like "Restaurant Name Generator" 
48:17.


The Streamlit application can be run using the command "streamlit run Main.py" in the terminal, which will display the application with the specified title 
48:39.


A simple application is created with a title, and a Picker is added to pick a cuisine, using the sidebar in Streamlit to create a select box with options such as Indian, American, and Mexican, and this is done by giving a name to the box and adding the desired options 
48:59.


When a cuisine is picked, the value is stored in a variable called "Cuisine", and then a function is called to generate a restaurant name and list of menu items based on the selected cuisine, with the function returning the restaurant name and menu items 
50:04.


A stub function is written to check the wiring, and then the actual code is written in that function, with the restaurant name and menu items being displayed on the right-hand side of the application using UI controls such as st.header 
50:21.


The menu items are split into a list using the split function with a comma separator, and then iterated over to display each item, with the option to add characters to indicate that it is an item 
52:08.


The code is then modified to use a modular approach, with a new Python file called "Lang_chain_Helper" created to hold the function, and the function is imported and called in the main application 
53:23.


A separate file is created to hold the secret key, which is imported and used in the application, with the key being used to access the language model 
54:01.


End-to-End Project 1: Equity Research Tool 
01:14:52

The project involves building an end-to-end LLM project that covers a real-life industry use case of equity research analysis, where a news research tool is created to retrieve answers based on given news articles 
01:14:52.


The technology used in this project includes Lang chain, Open Ai, and Streamlet, with an added storytelling element to make it more interesting, such as the story of Rocky B and his recruitment team, including Peter Pande, an equity research analyst 
01:15:11.


In the story, Peter Pande reads news articles for research, but Rocky B wants a chatbot like Chat GPT for his investment, which is similar to the role of equity research analysts in real life, such as those working for mutual funds 
01:15:33.


Mutual funds invest money in individual stocks and have a team of research analysts to provide research on companies, such as Tata Motors and Reliance, to determine which stocks to buy 
01:16:13.


The job of a research analyst, like Peter Pande, is to read news articles from various sources, such as Money Control and Economic Times, and do research based on the news articles, earning reports, and quarterly financial reports 
01:16:50.


The task of reading news articles from various websites is tedious, which is why building a tool that can retrieve answers based on given news articles is useful, such as a tool that can summarize articles or provide specific answers 
01:17:12.


The tool can be used by companies, such as Jeffrey's Open Hammer, and is a real industry use case, not just a toy project, with the goal of creating a chatbot that can answer questions based on given news articles 
01:18:30.


The technical architecture of the project involves building an LLM app, and the first consideration is whether Chat GPT can be used for this purpose, with the possibility of using it to answer questions based on given news articles 
01:19:10.


Equity research analysts are busy and do not have time to copy and paste articles to get answers, and they also need an aggregate knowledge base because the answer to their question might be in any article, so they need a tool that can pull all relevant articles into a knowledge base 
01:19:44.


There are three issues with using chat GPT for this purpose: copy-pasting articles is tedious, chat GPT cannot provide an aggregate knowledge base, and it has a word limit, so a tool is needed that can go to news websites, pull articles into a knowledge base, and build a chatboard that can pull data from that knowledge base 
01:20:04.


When building a tool that calls an open AI API, there is a cost associated with it per thousand tokens, and supplying more text increases the cost, but it is possible to save money by figuring out which chunk of text is relevant to the question and only supplying that chunk 
01:21:38.


The answer to a question may be present in a specific chunk of text, and when building a prompt, it is not necessary to give all the chunks, but only the relevant ones, and this can be done by finding the relevant chunks where the answer might be present 
01:22:35.


Finding relevant chunks cannot be done using direct keyword search, and a concept of semantic search is needed, which can understand the context and meaning of the search query, such as distinguishing between "apple" the fruit and "Apple" the company 
01:23:36.


Semantic search is used in NLP applications to figure out the meaning of a word based on the context, such as determining whether "apple" refers to a fruit or a company, and this is achieved using word embedding, sentence embedding, and a vector database 
01:23:59.


Embeddings and vector databases allow for faster search and retrieval of relevant information, enabling the use of prompts to get answers to questions, with embeddings helping to figure out relevant chunks and vector databases performing faster searches 
01:24:50.


The technical architecture of an NLP application involves a document loader to load news articles, splitting the articles into chunks, and storing them in a vector database for faster search and retrieval 
01:25:10.


Vector databases are designed to perform faster searches, even with millions of records, and are used to retrieve relevant chunks of information based on a given question, which is then used to form a prompt and get an answer 
01:25:45.


The application is built using classes and is initially developed in Streamlit as a proof of concept, with a long-term architecture involving a database injection system and a chatboard 
01:26:02.


The database injection system involves building a web scraper to pull data from trustworthy news article websites, converting the text into embedding vectors, and storing them in a vector database, with popular solutions including Pinecode, Milvus, and Chroma 
01:26:57.


The chatboard component is built using a UI framework like React, where a person's question is converted into an embedding, and relevant chunks are pulled from the vector database to form a prompt and get an answer from a language model 
01:27:34.


When working as a NLP engineer or data scientist in industry, the overall architecture is first brainstormed with the team, and then coding begins, with the goal of avoiding incorrect directions 
01:28:12.


To work with the Lang Chain Library, it is assumed that a basic overview has been obtained through the Lang Chain crash course, and the next step is to install Lang Chain by running the command "pip install Lang chain" 
01:28:49.


Once Lang Chain is installed, a Jupyter notebook can be launched, and the class can be imported, for example, by saying "from Lang_chain.document_loaders import TextLoader" to import a simple text loader 
01:29:07.


The text loader allows data to be loaded from a text file, and an example is shown using a text file called "nvda_news_1.txt", which is loaded using the "load" method and returns a data object 
01:29:44.


The data object is an array, and its zeroth element is a document that has page content and metadata as its elements, where page content is the entire text content and metadata is the name of the text file 
01:30:22.


The Lang Chain documentation provides information on various loader classes, including the text loader and the CSV loader, which can be used to load data from a CSV file 
01:31:02.


The CSV loader is demonstrated using a CSV file called "movies.csv", which has nine records, and the loader returns a data object with nine records, each with a document class that has page content and metadata 
01:31:42.


The page content of the CSV loader is the entire record in the CSV file, and the metadata is the name of the CSV file, but it can be argued that the metadata could be customized to include other information, such as the movie name or ID 
01:32:43.


The project utilizes a metadata system to reference source links, with a source column that can be designated as a title or movie ID, and this metadata is used to retrieve information such as the title "kgf2" for the first record 
01:33:04.


The unstructured URL loader is a class used to load text content from news articles directly into a Jupyter notebook, and it requires the installation of several libraries using a specific command 
01:34:24.


The unstructured URL loader uses the "unstructured" library to extract information from a website's HTML structure, and it can be created by supplying URLs of articles to be loaded 
01:35:04.


The loaded data can be retrieved using the "do_load" method, which returns a list of page contents and metadata, including the source URL link 
01:36:07.


The loaded documents need to be split into smaller chunks using classes like character TX splitter or recursive tax splitter, due to the token size limit of large language models 
01:36:25.


The splitting process may result in chunks that are not close to the token limit, so a merge step is necessary to combine smaller chunks and make them more efficient 
01:37:02.


Additionally, chunk overlapping is performed to provide context between chunks, allowing a portion of one chunk to be included in another, such as when reading a paragraph that needs context from a previous one 
01:37:38.


End-to-End Project 2: Retail Q&A Tool 
02:28:29

The project involves building an end-to-end LLM project where a tool similar to Chat GPT is created to convert natural human language questions into SQL queries and execute them on a MySQL database, giving the user a feeling of talking to the database in plain English language 
02:28:49.


The project is based on a t-shirt store called "At Te" that sells four main brands: Van Hussein, Leis, Nike, and Adidas, with its data stored in a MySQL database containing tables such as "t-shirts" for inventory count and "discounts" for applicable discounts on certain t-shirts 
02:29:07.


The store manager, Tony Sharma, uses software built on top of the MySQL database to answer questions, but often has custom questions that the software cannot handle, requiring him to either manually download data in Excel or ask the data analyst, Loki, for assistance 
02:30:18.


Loki, the data analyst, knows SQL and can run queries on the database, but is often busy building Power BI dashboards and may not be available to help Tony, who does not know SQL himself 
02:30:37.


The data scientist, Peter P, is asked to build a tool similar to Chat GPT that can convert human language questions into SQL queries and execute them on the database, using technologies such as Google Palm from the Lang Chain framework 
02:31:30.


The technical architecture of the tool involves using Google Palm to convert questions into SQL queries, and a SQL database chain class within the Lang Chain framework to execute the queries, with additional handling for complex queries using few-shot learning 
02:32:06.


Few-shot learning involves preparing a training dataset with sample questions and corresponding SQL queries, which can be done with the help of the data analyst, Loki, and does not require a large number of samples 
02:32:44.


The process of creating a Gen AI project involves converting a training data set into embedding vectors, which can be understood by watching videos on word embedding and sentence embedding, and this conversion will utilize the Hugging Face Library 
02:33:03.


The embedding vectors will be stored in a vector database, with options including Pinecone, MERS, Chroma, and others, but Chroma will be used for this project because it is open-source 
02:33:38.


The project will also involve pairing the vector database with Google PaLM, using a few short prompt templates to create an SQL database chain, and building a UI in Streamlit, which will require writing only a few lines of code 
02:33:56.


To work on this project, it is necessary to have a clear understanding of Langchain basics, as well as knowledge of what a vector database is, and there are resources available to learn about these topics 
02:34:16.


Google PaLM is one of three popular options for building LLM applications, along with OpenAI's GP4 model and Meta's LLaMA, but Google PaLM is free and easy to set up, making it the chosen option for this project 
02:34:35.


Setting up Google PaLM requires creating an API key, which can be done by logging into the Google Cloud website, going to the API key section, and creating a new key, and this API key should be kept safe and secure 
02:35:35.


The Google PaLM API key can be tested using the MakerSuite website, which provides a taste pad for trying different prompts, and allows for adjusting the creativity parameter to control the level of creativity in the responses 
02:36:13.


With the API key set up, the next step is to set up a MySQL database, which can be done using MySQL Workbench, and for those who are not familiar with MySQL, there are resources available to learn the basics 
02:36:54.


A complete idea for beginners to learn MySQL is provided, and the MySQL Workbench tool is recommended as it is free and can be easily downloaded by searching for it on Google, with all code files available in the video description 
02:37:12.


The MySQL Workbench tool allows users to create a local instance, and a database directory can be accessed where an SQL file can be dragged and dropped to create tables and data, which can be executed and refreshed to view the database, in this case, the "atck t-shirt" database 
02:37:33.


The "atck t-shirt" database has several tables, including the "T-shirts" table, which contains sample records with information such as t-shirt ID, color, size, price, and stock quantity, as well as discounts, for example, a 10% discount on a specific t-shirt 
02:38:11.


The records in the database will be different when the SQL script is executed because random numbers are used, and the database is now set up for coding in a Jupyter Notebook 
02:38:53.


In the Jupyter Notebook, the Google PAL model from Langchain can be imported and used to create an object, which requires a Google API key stored in a variable, and this object can be used to ask sample prompts, such as writing a poem 
02:39:12.


Before running the code, it is necessary to ensure that all libraries are installed by running the command "pip install -r requirements.txt", which includes requirements such as Langchain and chromaDB 
02:40:15.


An SQL database object can be created by importing a specific class and passing a URI that specifies the database, host, username, password, and other information, which can be stored in different variables, and this object has a property called "DB info" that can be printed to confirm the connection to the MySQL database 
02:41:18.


A Jupyter notebook is connected to a database, allowing for the creation of a SQL database chain using the Lang chain experimental module, which may be imported directly from a Linkchain module in the future 
02:42:21.


The SQL database chain is one of many chains available in Lang chain, each designed for different use cases, and it is imported from the experimental module, with the possibility of being moved to a stable module later 
02:42:40.


A chain object is created with parameters including the model and the DB object, and an additional parameter "verbose" is set to true to display the generated SQL query and internal details 
02:43:19.


The first query asks for the number of Nike white color extra small size t-shirts, and the model generates a correct query, pulling the right answer, which is 59, and the query can be verified by running it directly 
02:43:42.


The model is able to map natural language queries to SQL queries, handling nuances such as mapping "extra small" to the "size" column and "white" to the "color" column, which starts with a capital letter in the database 
02:45:00.


A second query asks for the price of the inventory for all small size t-shirts, and the model generates a query, but it returns an incorrect answer because it forgets to multiply the price by the stock quantity 
02:45:58.


The incorrect answer is due to the model's assumption that the "price" column refers to the total price, rather than the price per unit, highlighting the importance of clear column names and understanding the model's limitations 
02:46:59.


The model's ability to generate SQL queries from natural language input is demonstrated, but it also shows that the model can make mistakes if the input is ambiguous or if it makes incorrect assumptions about the data 
02:47:18.


The conclusion is that Large Language Models (LLMs) will make mistakes and need to be informed that the price column is price per unit, not the total price, which can be done using few-shot learning, and this is representing a real-life scenario where database column names are not perfect 
02:47:59.


To get the right answer, an explicit query can be run, which involves multiplying the price by stock quantity and summing it up to get the total revenue, and then applying discounts from the Discount table, which is stored in the "qns 2" variable 
02:48:33.


A more complex query is run to determine the revenue generated if all Levis t-shirts are sold with discounts, which involves joining the tables and applying discounts, but the LLM fails because it assumes the presence of start and end date columns for discounts, which are not present in the database 
02:49:14.


The LLM needs to be told to only use existing columns in the table and not rely on its general knowledge, which can be achieved using few-shot learning, and the correct query to get the answer involves a subquery and a left join with the Discount table 
02:50:17.


The query to get the revenue generated from selling all Levis t-shirts with discounts is run explicitly, and the result is stored in the "qns 3" variable, which is 24367, and other queries are run to determine the revenue generated from selling all Levis t-shirts and the number of white color Levis t-shirts 
02:51:15.


The query to find the number of white color Levis t-shirts involves filtering the results to only include t-shirts with a color equal to "White", and the total number of white Levis t-shirts is determined 
02:52:31.


The results of the queries are stored in variables, such as "qns 2" and "qns 3", which contain the answers to the questions, including the revenue generated from selling all Levis t-shirts with discounts and the number of white color Levis t-shirts 
02:52:51.


